# Промт для Lovable: SEO + AI visibility

Ни один промт не «включит индексацию по кнопке». Можно только сделать сайт максимально читаемым для поисковиков и AI-краулеров — а когда и что индексировать, решают они сами.

Ниже готовый промт для Lovable и краткое объяснение, что именно помогает **SEO (Google/Bing)** и **AI-видимости (GEO / AI search)**.

---

## Что нужно для индексации и видимости в AI

### 1) Чтобы индексировали Google/Bing

- **Нормальная мета-разметка на каждой странице** (title/description, OG/Twitter).
- **sitemap.xml + robots.txt** (и чтобы robots не блокировал важные разделы).
- **Кастомный домен и primary domain**, затем **верификация в Google Search Console**.

### 2) Чтобы AI-сервисы легче цитировали и понимали

- Контент должен быть **доступен в HTML без «всё на клиенте»** (SSR/SSG).
- **Структурированные данные (JSON-LD)**: Organization, Product/Service, FAQ, Article.
- **/llms.txt** — оглавление для LLM (не гарантия, но дешёвый и полезный шаг).
- Не блокировать AI-краулеров в robots.txt (не ставить `Disallow: /` на всё).

---

## Готовый промт для Lovable (копируй как есть)

> Сделай SEO+AI-friendly сайт. Требования:
>
> 1. Все страницы должны быть SSR/SSG, контент доступен в HTML без обязательного JS.
> 2. Для каждой страницы задай уникальные: `<title>` (до 60 символов), `meta description` (до 160), canonical URL, OpenGraph и Twitter cards.
> 3. Добавь структурированные данные JSON-LD: Organization (на главной), WebSite + SearchAction, FAQPage (на страницах FAQ), Article (для статей/кейсов), BreadcrumbList.
> 4. Создай `sitemap.xml` в корне, включи все важные URL, обновление при билде.
> 5. Создай `robots.txt` в корне: разреши индексацию, добавь ссылку на sitemap. Не блокируй важные разделы.
> 6. Создай `llms.txt` в корне (формат markdown): краткое описание проекта, список ключевых страниц (Docs/Services/Pricing/FAQ/Contacts), и «какие страницы цитировать в ответах».
> 7. Чистые URL (без мусора), нормальные H1-H2, внутренняя перелинковка, страницы «О нас», «Услуги/Продукт», «Цены», «FAQ», «Контакты».
> 8. Производительность: оптимизируй Core Web Vitals, изображения, lazy-load где надо.
> 9. После деплоя покажи ссылки на `https://мойдомен/sitemap.xml`, `https://мойдомен/robots.txt`, `https://мойдомен/llms.txt`.

---

## Если нужен «поиск ИИ» внутри сайта

Это отдельная фича: **встроенный поиск на embeddings**. Тогда добавь в промт:

> 10. Добавь on-site AI search: страница `/search` + API `/api/search`. Индексируй контент страниц (заголовок, описание, текст) в векторное хранилище (например Supabase pgvector). Поиск: hybrid (keyword + vector), выдача топ-результатов со ссылками. Обновление индекса при публикации/деплое.

---

## После того как Lovable всё сделал (must-do)

1. Подключить **custom domain** и сделать его **primary**.
2. Добавить сайт в **Google Search Console** и отправить sitemap.
3. Проверить, что реально открываются:

- `/sitemap.xml`
- `/robots.txt`
- `/llms.txt`

---

## Реальность без иллюзий

- **llms.txt не гарантирует**, что ChatGPT/Perplexity будут цитировать сайт. Это только повышает удобство для AI-краулеров.
- «Ручной заявки на включение в индекс» для LLM нет — всё через обнаружение краулером.

Хочешь видимость в AI-ответах — делай **полезные страницы (FAQ, how-to, кейсы)**, **структурку**, **нормальную индексацию** и **контент, доступный без JS**. Да, скучно. Зато работает.
